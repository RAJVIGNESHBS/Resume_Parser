{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import warnings as wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg.filterwarnings(\"ignore\")\n",
    "skills = \"Data mining or extracting usable data from valuable data sources\n",
    "Using machine learning tools to select features, create and optimize classifiers\n",
    "Carrying out preprocessing of structured and unstructured data Enhancing data\n",
    "collection procedures to include all relevant information for developing analytic\n",
    "systems Processing, cleansing, and validating the integrity of data to be used for\n",
    "analysis Analyzing large amounts of information to find patterns and solutions\n",
    "Developing prediction systems and machine learning algorithms Presenting\n",
    "results in a clear manner Propose solutions and strategies to tackle business\n",
    "challenges Collaborate with Business and IT teams Become a Data Science\n",
    "Expert & Get Your Dream Job Professional Certificate Program In Data\n",
    "ScienceEXPLORE PROGRAMBecome a Data Science Expert & Get Your\n",
    "Dream Job Data Scientist Skills You need to master the skills required for data\n",
    "scientist jobs in various industries and organizations if you want to pursue a\n",
    "data scientist career. Let’s look at the must-have data scientist qualifications.\n",
    "Key skills needed to become a data scientist: Programming Skills – knowledge\n",
    "of statistical programming languages like R, Python, and database query\n",
    "languages like SQL, Hive, Pig is desirable. Familiarity with Scala, Java, or C++\n",
    "is an added advantage. Statistics – Good applied statistical skills, including\n",
    "knowledge of statistical tests, distributions, regression, maximum likelihood\n",
    "estimators, etc. Proficiency in statistics is essential for data-driven companies.\n",
    "Machine Learning – good knowledge of machine learning methods like k-\n",
    "Nearest Neighbors, Naive Bayes, SVM, Decision Forests. Strong Math Skills\n",
    "(Multivariable Calculus and Linear Algebra) - understanding the fundamentals\n",
    "of Multivariable Calculus and Linear Algebra is important as they form the\n",
    "basis of a lot of predictive performance or algorithm optimization techniques.\n",
    "Data Wrangling – proficiency in handling imperfections in data is an important\n",
    "aspect of a data scientist job description. Experience with Data Visualization\n",
    "Tools like matplotlib, ggplot, d3.js., Tableau that help to visually encode data\n",
    "Excellent Communication Skills – it is incredibly important to describe findings\n",
    "to a technical and non-technical audience. Strong Software Engineering\n",
    "Background Hands-on experience with data science tools Problem-solving\n",
    "aptitude Analytical mind and great business sense Degree in Computer Science,\n",
    "Engineering or relevant field is preferred Proven Experience as Data Analyst or\n",
    "Data Scientist\"\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# First, join all the skills into a single string and remove punctuation\n",
    "text_nonpunc = \"\".join([char for char in skills if char not in string.punctuation])\n",
    "# Split the string into tokens (words)\n",
    "tokens = re.split(\"\\W+\", text_nonpunc)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "re_sw = [word for word in tokens if word not in stop_words]\n",
    "ln = nltk.WordNetLemmatizer()\n",
    "lemm = [ln.lemmatize(word) for word in re_sw]\n",
    "# Assign the lemmatized list back to the original variable\n",
    "skills = lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\" \".join([char for char in skills]))\n",
    "def tot_list(text):\n",
    "return set(\" \".join([char for char in text]).lower().split(\" \"))\n",
    "# Extract the skills from the spaCy Doc object by combining noun chunks,\n",
    "verbs, and named entities\n",
    "skills_list = set(\" \".join([chunk.text for chunk in doc.noun_chunks] +\n",
    "[token.lemma_ for token in doc if token.pos_ == \"VERB\"] + [entity.text for\n",
    "entity in doc.ents] ) .lower() .split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/UpdatedResumeDataSet.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/content/drive/MyDrive/after_remove_punctuation.csv'\n",
    "if os.path.isfile(file_name):\n",
    "os.remove(file_name)\n",
    "def remove_punctuation(text):\n",
    "text_punctuation = ''.join([char for char in text if char not in\n",
    "string.punctuation])\n",
    "return text_punctuation\n",
    "df['clean_punctuation'] = df['info'].apply(lambda x: remove_punctuation(x))\n",
    "df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(text):\n",
    "tokens = re.split('\\W+', text)\n",
    "return tokens\n",
    "df['after_tokenization'] = df['info'].apply(lambda x: token(x))\n",
    "output_file = '/content/drive/MyDrive/after_tokenization.csv'\n",
    "if os.path.exists(output_file):\n",
    "os.remove(output_file)\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def pos_tag(text):\n",
    "pos_text = nltk.pos_tag(text)\n",
    "return pos_text\n",
    "df['after_pos_tagging'] = df['after_tokenization'].apply(lambda x: pos_tag(x))\n",
    "output_file = '/content/drive/MyDrive/after_pos_tagging.csv'\n",
    "if os.path.exists(output_file):\n",
    "os.remove(output_file)\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopword(text):\n",
    "re_sw = [word for word in text if word not in stop_words]\n",
    "return re_sw\n",
    "df['remove_sw'] = df['after_tokenization'].apply(lambda x:\n",
    "remove_stopword(x))\n",
    "output_file = '/content/drive/MyDrive/after_re_sw.csv'\n",
    "if os.path.exists(output_file):\n",
    "os.remove(output_file)\n",
    "\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = nltk.WordNetLemmatizer()\n",
    "def lemm(text):\n",
    "z = [ln.lemmatize(word) for word in text]\n",
    "return z\n",
    "df['after_lemmatization'] = df['remove_sw'].apply(lambda x: lemm(x))\n",
    "output_file = '/content/drive/MyDrive/after_lemmatization.csv'\n",
    "if os.path.exists(output_file):\n",
    "os.remove(output_file)\n",
    "df.to_csv(output_file, index=False)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_phrases(text):\n",
    "x = ' '.join([y for y in text])\n",
    "doc = nlp(x)\n",
    "return [chunk.text for chunk in doc.noun_chunks]\n",
    "df['noun_phrases'] = df['after_lemmatization'].apply(lambda x:\n",
    "noun_phrases(x))\n",
    "print(df['noun_phrases'])\n",
    "output_file = '/content/drive/MyDrive/after_noun_phrases.csv'\n",
    "if os.path.exists(output_file):\n",
    "os.remove(output_file)\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_forms(text):\n",
    "x = ' '.join([y for y in text])\n",
    "doc = nlp(x)\n",
    "return [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "df['verb_forms'] = df['after_lemmatization'].apply(lambda x: verb_forms(x))\n",
    "df['verb_forms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entities(text):\n",
    "x=' '.join([y for y in text])\n",
    "doc=nlp(x)\n",
    "return [entity.text for entity in doc.ents]\n",
    "df['named_entities']=df['after_lemmatization'].apply(lambda\n",
    "x:named_entities(x))\n",
    "df['named_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string(text):\n",
    "return set(' '.join([char for char in text]).lower().split(' '))\n",
    "df['total_words'] = (df['noun_phrases'] + df['verb_forms'] +\n",
    "df['named_entities']).apply(lambda x: string(x))\n",
    "df['total_words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "def ct(text):\n",
    "count = 0\n",
    "for i in text:\n",
    "if i in skills_list:\n",
    "count += 1\n",
    "return count\n",
    "df['skills_matched'] = df['total_words'].apply(lambda x: ct(x))\n",
    "df\n",
    "brandname=\"Resume\"\n",
    "df.to_excel(brandname + 'Updated_resumes.xlsx')\n",
    "files.download(brandname + 'Updated_resumes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='skills_matched', inplace=True, ascending=False)\n",
    "top_resumes = df[['Category', 'Resume', 'skills_matched']].head()\n",
    "print(top_resumes)\n",
    "top_resumes.to_csv(brandname + 'top_resumes.csv', sep='\\t')\n",
    "files.download(brandname + 'Updated_resumes.csv')\n",
    "(Resumes which skills matched more than 40)\n",
    "ar=df[df['skills_matched']>=40][['Category','Resume','skills_matched']]\n",
    "randname=\"Top40_skills\"\n",
    "ar.to_excel(randname + '_resumes.xlsx')\n",
    "files.download(randname + '_resumes.xlsx')\n",
    "len(df[df['skills_matched']>=40][['Category','Resume','skills_matched']])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
